I evaluated the perplexity on a number of different models like google/gemma-2b, TinyLlama/TinyLlama_v1.1, meta-llama/Llama-2-7b-hf and microsoft/Phi-3-mini-4k-instruct.
I used the standard perplexity function from the evaluate library as well as wrote down a couple of custom functions in which I experimented evaluating perplexity with varying context window sizes.
